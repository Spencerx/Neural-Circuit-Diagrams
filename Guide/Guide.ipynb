{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a guide to drawing Neural Circuit Diagrams by Vincent Abbott from the paper [*Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures*](https://openreview.net/forum?id=RyZB4qXEgt). It allows for deep learning algorithms to be comprehensively expressed using a novel diagrammatic scheme.\n",
    "\n",
    "This is the Mathcha component of the guide. Implementations can be found in the [Mathcha](https://www.mathcha.io/editor/p8KjdC6yI3nH7VLEpfLoK4lOSNWnNl1hNNOmp3) portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Components.\n",
    "We begin with an overview of the basics of neural circuit diagrams. Wires represent axes, dashed lines represent tuples, and operations change the shape of data and are represented as symbols or pictograms. Axes can be drawn to broadcast operations.\n",
    "\n",
    "<img src=\"PNG/BasicComponents.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines.\n",
    "We use [mathcha.io](https://www.mathcha.io/editor) (math-*cha*) to make and modify diagrams. There are standard settings for snapping and managing diagrams which make the process of creating diagrams easier.\n",
    "\n",
    "<img src=\"PNG/Guidelines.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation.\n",
    "Diagrams are in close correspondence to implementation. This means that once a diagram is made, implementing an algorithm is straightforward. Here, I have included an implementation of the above diagram as an example.\n",
    "\n",
    "<img src=\"PNG/TwoLayer.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor as T\n",
    "from typing import Tuple, Any\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerResidualMLP(nn.Module):\n",
    "    \"\"\" A feed-forward layer consisting of two learned linear layers.\"\"\"\n",
    "    def __init__(self, xbar : int, m: int, dff : int,\n",
    "        device : Any | None = None, dtype : Any | None = None) -> None:\n",
    "        super().__init__()\n",
    "        self.xbar, self.m, self.dff = xbar, m, dff\n",
    "        # Bold (learned) components must be initialized.\n",
    "        # The + indicates that learned linear layers have bias.\n",
    "        bias = True\n",
    "        self.L0 = nn.Linear(m, dff, bias, device, dtype)\n",
    "        self.L1 = nn.Linear(dff, m, bias, device, dtype)\n",
    "    \n",
    "    def forward(self, x : T):\n",
    "        \"\"\" ... m -> ... m \"\"\"\n",
    "        # We keep \"x\" for an implicit copy.\n",
    "        # Linear layers are applied onto the lowest axis.\n",
    "        x1 = self.L0(x)\n",
    "        x1 = nn.functional.relu(x1)\n",
    "        x1 = self.L1(x1)\n",
    "        x1 = x1 + x\n",
    "        return x1\n",
    "\n",
    "# Now we can run it on fake data;\n",
    "xbar, m, dff = 256, 1024, 4096\n",
    "# Input Data\n",
    "x = torch.rand((xbar, m))\n",
    "\n",
    "ff = TwoLayerResidualMLP(xbar, m, dff)\n",
    "assert tuple(ff.forward(x).size()) == (xbar, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein Operations.\n",
    "By keeping operation and shape columns separate, we can easily diagram more complex algorithms in Mathcha. Here, we diagram Multi-Head Attention, an algorithm which uses multiple Einstein operations. The interaction between axes is clearly shown with diagrams.\n",
    "\n",
    "*(This is a subsection of the [full transformer diagram](https://twitter.com/jxmnop/status/1757244005639766157).)*\n",
    "\n",
    "<img src=\"PNG/Einops.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilinear(nn.Module):\n",
    "    \"\"\" A learned linear linear which supports tuple axis sizes. \"\"\"\n",
    "    def __init__(self, in_size  : Tuple[int] | int, out_size : Tuple[int] | int, \n",
    "        bias : bool = True, device : Any | None = None, dtype : Any | None = None) -> None:\n",
    "        super().__init__()\n",
    "        # Set the parameters\n",
    "        get_size = lambda x: (x, math.prod(x)) \\\n",
    "            if isinstance(x, tuple) else ((x,), x)\n",
    "        self.in_size,  self.in_features  = get_size(in_size)\n",
    "        self.out_size, self.out_features = get_size(out_size)\n",
    "        # Set up the linear module\n",
    "        self.linear = nn.Linear(self.in_features, self.out_features, bias, device, dtype)\n",
    "    \n",
    "    def forward(self, x_in : torch.Tensor):\n",
    "        # Reshape the input. The last axes should match, else there's an error.\n",
    "        x_in = x_in.reshape(\n",
    "            x_in.shape[:-len(self.in_size)] + (self.in_features,))\n",
    "        # Apply the linear over the last axis.\n",
    "        x = self.linear(x_in)\n",
    "        # Return the proper output.\n",
    "        return x.reshape(\n",
    "            x.shape[:-1] + self.out_size)\n",
    "\n",
    "def MultiHeadDotProductAttention(q: T, k: T, v: T) -> T:\n",
    "    # In practice, we add dots to consider batched axes.\n",
    "    ''' ... y k h, ... x k h, ... x k h -> ... y k h '''\n",
    "    klength = k.size()[-2]\n",
    "    x = einops.einsum(q, k, '... y k h, ... x k h -> ... y x h')\n",
    "    x = torch.nn.Softmax(-2)(x / math.sqrt(klength))\n",
    "    x = einops.einsum(x, v, '... y x h, ... x k h -> ... y k h')\n",
    "    return x\n",
    "\n",
    "# We implement this component as a neural network model.\n",
    "# This is necessary when there are bold, learned components that need to be initialized.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Multi-Head attention has various settings, which become variables\n",
    "    # for the initializer.\n",
    "    def __init__(self, m, k, h):\n",
    "        super().__init__()\n",
    "        self.m, self.k, self.h = m, k, h\n",
    "        # Set up all the boldface, learned components\n",
    "        self.Lq = Multilinear(m, (k,h), False)\n",
    "        self.Lk = Multilinear(m, (k,h), False)\n",
    "        self.Lv = Multilinear(m, (k,h), False)\n",
    "        self.Lo = Multilinear((k,h), m, False)\n",
    "\n",
    "\n",
    "    # We have endogenous data (y) and external / injected data (x)\n",
    "    def forward(self, y : T, x : T):\n",
    "        \"\"\" ... ybar m, ... xbar m -> ... ybar m \"\"\"\n",
    "        # We first generate query, key, and value vectors.\n",
    "        # Linear layers are automatically broadcast.\n",
    "        q = self.Lq(y)\n",
    "        k = self.Lk(x)\n",
    "        v = self.Lv(x)\n",
    "\n",
    "        # We feed q, k, and v to standard multi-head inner product attention\n",
    "        o = MultiHeadDotProductAttention(q, k, v)\n",
    "        return self.Lo(o)\n",
    "\n",
    "# Now we can run it on fake data;\n",
    "ybar, xbar, m, k, h = 20, 22, 128, 16, 4\n",
    "# Internal Data\n",
    "y = torch.rand((ybar, m))\n",
    "# External Data\n",
    "x = torch.rand((xbar, m))\n",
    "\n",
    "mha = MultiHeadAttention(m,k,h)\n",
    "assert tuple(mha.forward(y, x).size()) == (ybar, m)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
